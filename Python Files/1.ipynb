{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":574,"status":"ok","timestamp":1710825686327,"user":{"displayName":"Ded Sec","userId":"09769380213393744986"},"user_tz":-330},"id":"onIjVQqqJ2z0"},"outputs":[{"name":"stdout","output_type":"stream","text":["/physical_device:GPU:0\n","/physical_device:GPU:1\n"]}],"source":["import tensorflow as tf  # Importing TensorFlow library for deep learning operations\n","import numpy as np  # Importing NumPy library for numerical operations\n","import os  # Importing os module for interacting with the operating system\n","import cv2  # Importing OpenCV library for image processing\n","from time import time as time  # Importing time function from time module to calculate time taken for training\n","gpus = tf.config.list_physical_devices('GPU')\n","for gpu in gpus:\n","    print(gpu.name)"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":580,"status":"ok","timestamp":1710825772125,"user":{"displayName":"Ded Sec","userId":"09769380213393744986"},"user_tz":-330},"id":"dSC6-dqiJ2z3"},"outputs":[],"source":["#train_path = os.path.join(os.getcwd(), \"Train/\")  # Constructing the path for the training images directory\n","#test_path = os.path.join(os.getcwd(), \"Test/\")  # Constructing the path for the testing images directory\n","train_path = os.path.join(os.getcwd(), \"C:/Users/vedan/Desktop/NUV/Tyre_Guardian/images/Train/\")\n","test_path = os.path.join(os.getcwd(), \"C:/Users/vedan/Desktop/NUV/Tyre_Guardian/images/Test/\")"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":675,"status":"ok","timestamp":1710825788404,"user":{"displayName":"Ded Sec","userId":"09769380213393744986"},"user_tz":-330},"id":"uhpsaOabJ2z3"},"outputs":[],"source":["def load_folder(path):\n","    x_data = []  # List to store images\n","    y_data = []  # List to store labels\n","    for folder in os.listdir(path):  # Iterating through each folder in the specified directory\n","        label = 0 if folder == \"defective\" else 1  # Assigning label 1 if folder name is \"defective\", else 0\n","        folder_path = os.path.join(path, folder)  # Constructing the path for the current folder\n","\n","        for img in os.listdir(folder_path):  # Iterating through each image in the folder\n","            image_path = os.path.join(folder_path, img)  # Constructing the path for the current image\n","            image = cv2.imread(image_path)  # Reading the image using OpenCV\n","            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Converting image to RGB color space\n","            #image = cv2.resize(image, (256, 256))  # Resizing the image to (256, 256)\n","            x_data.append(image)  # Appending the image to the list of images\n","            y_data.append(label)  # Appending the label to the list of labels\n","    return np.array(x_data), np.array(y_data)  # Converting lists to NumPy arrays and returning"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":132036,"status":"ok","timestamp":1710825923890,"user":{"displayName":"Ded Sec","userId":"09769380213393744986"},"user_tz":-330},"id":"4rDNaam6J2z4"},"outputs":[{"name":"stdout","output_type":"stream","text":["(2307, 256, 256, 3) (2307,) (577, 256, 256, 3) (577,)\n"]}],"source":["x_train, y_train = load_folder(train_path)  # Loading training images and labels\n","x_test, y_test = load_folder(test_path)  # Loading testing images and labels\n","print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)  # Printing shapes of the loaded data"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":774,"status":"ok","timestamp":1710825935102,"user":{"displayName":"Ded Sec","userId":"09769380213393744986"},"user_tz":-330},"id":"h-d-OZppJ2z4","outputId":"b5ad57dd-c8b4-47b2-a363-b0c911c1722b"},"outputs":[],"source":["np.save(\"x_train.npy\", x_train)  # Saving training images as numpy array\n","np.save(\"y_train.npy\", y_train)  # Saving training labels as numpy array\n","np.save(\"x_test.npy\", x_test)  # Saving testing images as numpy array\n","np.save(\"y_test.npy\", y_test)  # Saving testing labels as numpy array"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def load_data():\n","    x_train = np.load(\"x_train.npy\")  # Loading training images\n","    y_train = np.load(\"y_train.npy\")  # Loading training labels\n","    x_test = np.load(\"x_test.npy\")  # Loading testing images\n","    y_test = np.load(\"y_test.npy\")  # Loading testing labels\n","\n","    return x_train, y_train, x_test, y_test  # Returning the loaded data"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(2307, 256, 256, 3) (2307,) (577, 256, 256, 3) (577,)\n"]}],"source":["x_train, y_train, x_test, y_test = load_data()  # Loading data\n","print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)  # Printing shapes of the loaded data"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# preprocessing\n","y_train.reshape(y_train.shape[0], 1)  # Reshaping training labels\n","y_test.reshape(y_test.shape[0], 1)  # Reshaping testing labels\n","x_train = x_train.astype('float32') / 255.0  # Normalizing training images\n","x_test = x_test.astype('float32') / 255.0  # Normalizing testing images"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/50\n","73/73 [==============================] - 8s 70ms/step - loss: 0.6953 - accuracy: 0.5340\n","Epoch 2/50\n","73/73 [==============================] - 5s 63ms/step - loss: 0.6696 - accuracy: 0.5869\n","Epoch 3/50\n","73/73 [==============================] - 5s 63ms/step - loss: 0.6358 - accuracy: 0.6441\n","Epoch 4/50\n","73/73 [==============================] - 5s 63ms/step - loss: 0.6293 - accuracy: 0.6511\n","Epoch 5/50\n","73/73 [==============================] - 5s 63ms/step - loss: 0.6080 - accuracy: 0.6775\n","Epoch 6/50\n","73/73 [==============================] - 5s 64ms/step - loss: 0.6013 - accuracy: 0.6736\n","Epoch 7/50\n","73/73 [==============================] - 5s 64ms/step - loss: 0.5850 - accuracy: 0.6797\n","Epoch 8/50\n","73/73 [==============================] - 5s 65ms/step - loss: 0.5654 - accuracy: 0.6931\n","Epoch 9/50\n","73/73 [==============================] - 5s 64ms/step - loss: 0.5829 - accuracy: 0.6805\n","Epoch 10/50\n","73/73 [==============================] - 5s 63ms/step - loss: 0.5232 - accuracy: 0.7213\n","Epoch 11/50\n","73/73 [==============================] - 5s 64ms/step - loss: 0.4924 - accuracy: 0.7525\n","Epoch 12/50\n","73/73 [==============================] - 5s 64ms/step - loss: 0.4454 - accuracy: 0.7815\n","Epoch 13/50\n","73/73 [==============================] - 5s 64ms/step - loss: 0.4141 - accuracy: 0.8032\n","Epoch 14/50\n","73/73 [==============================] - 5s 64ms/step - loss: 0.4048 - accuracy: 0.8145\n","Epoch 15/50\n","73/73 [==============================] - 5s 64ms/step - loss: 0.3466 - accuracy: 0.8453\n","Epoch 16/50\n","73/73 [==============================] - 5s 64ms/step - loss: 0.3299 - accuracy: 0.8544\n","Epoch 17/50\n","73/73 [==============================] - 5s 64ms/step - loss: 0.2920 - accuracy: 0.8708\n","Epoch 18/50\n","73/73 [==============================] - 5s 64ms/step - loss: 0.3173 - accuracy: 0.8570\n","Epoch 19/50\n","73/73 [==============================] - 5s 64ms/step - loss: 0.2569 - accuracy: 0.8856\n","Epoch 20/50\n","73/73 [==============================] - 5s 64ms/step - loss: 0.2086 - accuracy: 0.9181\n","Epoch 21/50\n","73/73 [==============================] - 5s 64ms/step - loss: 0.1837 - accuracy: 0.9267\n","Epoch 22/50\n","73/73 [==============================] - 5s 64ms/step - loss: 0.1400 - accuracy: 0.9471\n","Epoch 23/50\n","73/73 [==============================] - 5s 64ms/step - loss: 0.1177 - accuracy: 0.9497\n","Epoch 24/50\n","73/73 [==============================] - 5s 64ms/step - loss: 0.1226 - accuracy: 0.9584\n","Epoch 25/50\n","73/73 [==============================] - 5s 64ms/step - loss: 0.1072 - accuracy: 0.9619\n","Epoch 26/50\n","73/73 [==============================] - 5s 64ms/step - loss: 0.0928 - accuracy: 0.9640\n","Epoch 27/50\n","73/73 [==============================] - 5s 64ms/step - loss: 0.0713 - accuracy: 0.9762\n","Epoch 28/50\n","73/73 [==============================] - 5s 64ms/step - loss: 0.0747 - accuracy: 0.9762\n","Epoch 29/50\n","73/73 [==============================] - 5s 64ms/step - loss: 0.0653 - accuracy: 0.9749\n","Epoch 30/50\n","73/73 [==============================] - 5s 64ms/step - loss: 0.0565 - accuracy: 0.9814\n","Epoch 31/50\n","73/73 [==============================] - 5s 64ms/step - loss: 0.0687 - accuracy: 0.9744\n","Epoch 32/50\n","73/73 [==============================] - 5s 64ms/step - loss: 0.0444 - accuracy: 0.9840\n","Epoch 33/50\n","73/73 [==============================] - 5s 65ms/step - loss: 0.0282 - accuracy: 0.9913\n","Epoch 34/50\n","73/73 [==============================] - 5s 65ms/step - loss: 0.0539 - accuracy: 0.9835\n","Epoch 35/50\n","73/73 [==============================] - 5s 64ms/step - loss: 0.0325 - accuracy: 0.9887\n","Epoch 36/50\n","73/73 [==============================] - 5s 64ms/step - loss: 0.0454 - accuracy: 0.9857\n","Epoch 37/50\n","73/73 [==============================] - 5s 64ms/step - loss: 0.0327 - accuracy: 0.9887\n","Epoch 38/50\n","73/73 [==============================] - 5s 65ms/step - loss: 0.0297 - accuracy: 0.9922\n","Epoch 39/50\n","73/73 [==============================] - 5s 64ms/step - loss: 0.0213 - accuracy: 0.9926\n","Epoch 40/50\n","73/73 [==============================] - 5s 64ms/step - loss: 0.0248 - accuracy: 0.9939\n","Epoch 41/50\n","73/73 [==============================] - 5s 64ms/step - loss: 0.0131 - accuracy: 0.9970\n","Epoch 42/50\n","73/73 [==============================] - 5s 64ms/step - loss: 0.0233 - accuracy: 0.9909\n","Epoch 43/50\n","73/73 [==============================] - 5s 64ms/step - loss: 0.0317 - accuracy: 0.9896\n","Epoch 44/50\n","73/73 [==============================] - 5s 64ms/step - loss: 0.0225 - accuracy: 0.9935\n","Epoch 45/50\n","73/73 [==============================] - 5s 65ms/step - loss: 0.0331 - accuracy: 0.9883\n","Epoch 46/50\n","73/73 [==============================] - 5s 64ms/step - loss: 0.0183 - accuracy: 0.9948\n","Epoch 47/50\n","73/73 [==============================] - 5s 64ms/step - loss: 0.0370 - accuracy: 0.9866\n","Epoch 48/50\n","73/73 [==============================] - 5s 64ms/step - loss: 0.0535 - accuracy: 0.9818\n","Epoch 49/50\n","73/73 [==============================] - 5s 64ms/step - loss: 0.0125 - accuracy: 0.9965\n","Epoch 50/50\n","73/73 [==============================] - 5s 65ms/step - loss: 0.0109 - accuracy: 0.9970\n","19/19 [==============================] - 1s 43ms/step - loss: 0.0837 - accuracy: 0.9740\n","Accuracy: 97.40034937858582%\n","Time to train on GPU 243.6919343471527\n"]}],"source":["# define model\n","'''model = tf.keras.Sequential([\n","    tf.keras.layers.Conv2D(64, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu', input_shape=(256, 256, 3)),  # Adding a 2D convolutional layer with 64 filters, kernel size 3x3, ReLU activation, and input shape (256, 256, 3)\n","    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),  # Adding a max pooling layer with pool size 2x2 and stride 2x2\n","    tf.keras.layers.Dropout(0.25),  # Adding a dropout layer with dropout rate 0.25\n","    tf.keras.layers.Conv2D(128, kernel_size=(3, 3), strides=(2, 2), activation='relu'),  # Adding another 2D convolutional layer with 128 filters, kernel size 3x3, ReLU activation, and stride 2x2\n","    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),  # Adding another max pooling layer with pool size 2x2 and stride 2x2\n","    tf.keras.layers.Dropout(0.25),  # Adding another dropout layer with dropout rate 0.25\n","    tf.keras.layers.Conv2D(64, kernel_size=(3, 3), strides=(2, 2), padding='same', activation='relu', input_shape=(256, 256, 3)),\n","    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),  # Adding a max pooling layer with pool size 2x2 and stride 2x2\n","    tf.keras.layers.Dropout(0.25),\n","    tf.keras.layers.Flatten(),  # Flattening the output from the convolutional layers\n","    tf.keras.layers.Dense(128, activation='relu'),  # Adding a fully connected dense layer with 128 units and ReLU activation\n","    tf.keras.layers.Dense(1, activation='sigmoid')  # Adding the output layer with 1 unit and sigmoid activation for binary classification\n","])\n","'''\n","#Relu= rectified Linear unit f(x)=max(0,x) introduces non linearity\n","#strides= step size of kernel (x,y)\n","#padding= add or same\n","#units= nodes\n","#dense= fully connected layer\n","with tf.device('GPU:0'):\n","    start=time()\n","    model = tf.keras.Sequential([\n","        tf.keras.layers.Conv2D(64, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu', input_shape=(256, 256, 3)),  # Adding a 2D convolutional layer with 64 filters, kernel size 3x3, ReLU activation, and input shape (256, 256, 3)\n","        tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),  # Adding a max pooling layer with pool size 2x2 and stride 2x2\n","        tf.keras.layers.Dropout(0.25),  # Adding a dropout layer with dropout rate 0.25\n","        tf.keras.layers.Conv2D(128, kernel_size=(3, 3), strides=(2, 2), activation='relu'),  # Adding another 2D convolutional layer with 128 filters, kernel size 3x3, ReLU activation, and stride 2x2\n","        tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),  # Adding another max pooling layer with pool size 2x2 and stride 2x2\n","        tf.keras.layers.Dropout(0.25),  # Adding another dropout layer with dropout rate 0.25\n","        tf.keras.layers.Conv2D(64, kernel_size=(3, 3), strides=(2, 2), padding='same', activation='relu', input_shape=(256, 256, 3)),\n","        tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),  # Adding a max pooling layer with pool size 2x2 and stride 2x2\n","        tf.keras.layers.Dropout(0.25),\n","        tf.keras.layers.Flatten(),  # Flattening the output from the convolutional layers\n","        tf.keras.layers.Dense(128, activation='relu'),  # Adding a fully connected dense layer with 128 units and ReLU activation\n","        tf.keras.layers.Dense(1, activation='sigmoid')  # Adding the output layer with 1 unit and sigmoid activation for binary classification\n","        ])\n","    model.compile(\n","        loss=tf.keras.losses.BinaryCrossentropy(),  # Using binary crossentropy loss function for binary classification\n","        optimizer=tf.keras.optimizers.Adam(0.001),  # Using Adam optimizer with learning rate 0.001 less rate as slower convergence of and low rate of parameter update\n","        metrics=['accuracy']  # Monitoring accuracy metric during training\n","        )\n","    model.fit(x_train, y_train, epochs=50)  # Training the model on the training data for 50 epochs\n","    print(f\"Accuracy: {model.evaluate(x_test, y_test)[1] * 100}%\")  # Evaluating and printing accuracy of the model\n","    end=time()\n","    print('Time to train on GPU',end-start)  # Printing time taken to train the model on GPU"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# save model\n","model.save(\"model.keras\")  # Saving the trained model"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"}},"nbformat":4,"nbformat_minor":0}
